# -*- coding: utf-8 -*-
"""LogitBoostModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15IigeQ25SJoJ-z-YaDM1DQhdq3yv7iJD
"""

from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.metrics import log_loss
import numpy as np
import time
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder

class Classifier2(BaseEstimator, ClassifierMixin):
    def __init__(self):
        self.model = LogisticRegression( max_iter=10000)

    def fit(self, X, y):
        self.model.fit(X, y)
        return self

    def predict(self, X):
        return self.model.predict(X)

    def predict_proba(self, X):
        return self.model.predict_proba(X)

# ---------------------
# Ensemble Model (modified to accept a factory for classifier1)
# ---------------------
class EnsembleModel(BaseEstimator, ClassifierMixin):
    def __init__(self, feature_subset1, feature_subset2, clf1_factory=None, clf2=None):
        """
        clf1_factory: a callable that returns a new classifier1 instance.
        clf2: an instance of classifier2 (or similar).
        """
        self.feature_subset1 = feature_subset1
        self.feature_subset2 = feature_subset2
        self.clf1 = clf1_factory() if clf1_factory is not None else None
        self.clf2 = clf2 if clf2 is not None else Classifier2()
        self.one_hot_encoder = OneHotEncoder(sparse_output=False)

    def fit(self, X, y):
        # Split features into two subsets
        X1 = X.iloc[:, self.feature_subset1]
        X2 = X.iloc[:, self.feature_subset2]

        # Fit first classifier (classifier1)
        self.clf1.fit(X1, y)
        y_pred1_proba = self.clf1.predict_proba(X1)
        link_scale1 = np.log(y_pred1_proba / (1 - y_pred1_proba))

        # One-hot encode true labels
        y_one_hot = self.one_hot_encoder.fit_transform(y.reshape(-1, 1))

        # Calculate residuals and use the max residual as target
        residuals = y_one_hot - y_pred1_proba
        self.clf2.fit(X2, residuals.argmax(axis=1))
        return self

    def predict_link_scale(self, X):
        X1 = X.iloc[:, self.feature_subset1]
        X2 = X.iloc[:, self.feature_subset2]

        y_pred1_proba = self.clf1.predict_proba(X1)
        link_scale1 = np.log(y_pred1_proba / (1 - y_pred1_proba))
        y_pred2_proba = self.clf2.predict_proba(X2)
        link_scale2 = np.log(y_pred2_proba / (1 - y_pred2_proba))
        return link_scale1 + link_scale2

    def predict(self, X):
        combined_link_scale = self.predict_link_scale(X)
        return np.argmax(combined_link_scale, axis=1)

    def score(self, X, y):
        return accuracy_score(y, self.predict(X))

    def mse(self, X, y):
        return mean_squared_error(y, self.predict(X))

class LogitBoost(BaseEstimator, ClassifierMixin):
    def __init__(self, n_estimators=50, learning_rate=0.1, 
                 feature_subset1=None, feature_subset2=None, 
                 clf1_factory=None):
        """
        feature_subset1, feature_subset2: feature indices for splitting the data.
        clf1_factory: a callable that returns a new instance of your classifier1.
        """
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.feature_subset1 = feature_subset1
        self.feature_subset2 = feature_subset2
        self.clf1_factory = clf1_factory
        self.estimators_ = []

    def fit(self, X, y):
        n_samples, _ = X.shape
        self.classes_ = np.unique(y)
        self.K = len(self.classes_)

        F = np.zeros((n_samples, self.K))
        P = np.ones((n_samples, self.K)) / self.K

        # One-hot encode labels
        Y = np.zeros((n_samples, self.K))
        for i, class_ in enumerate(self.classes_):
            Y[:, i] = np.where(y == class_, 1, 0)

        start_time = time.time()
        best_log_loss = 1e+20
        log_loss_train=np.zeros(self.n_estimators)
        mse_train=np.zeros(self.n_estimators)
        accuracy_train=np.zeros(self.n_estimators)
        for m in range(self.n_estimators):
            iteration_start_time = time.time()
            residuals = Y - P
            estimator = EnsembleModel(self.feature_subset1, self.feature_subset2,
                                      clf1_factory=self.clf1_factory)
            # Use class with max residual as a proxy target
            estimator.fit(X, residuals.argmax(axis=1))
            self.estimators_.append(estimator)

            F += self.learning_rate * estimator.predict_link_scale(X)
            F = np.clip(F, -25, 25)
            P = np.exp(F) / np.exp(F).sum(axis=1, keepdims=True)
            P = np.nan_to_num(P, nan=1e-10)

            log_loss_value = log_loss(y, P)
            if log_loss_value < best_log_loss:
                best_log_loss = log_loss_value
                self.best_model = m + 1
            y_pred_train = self.classes_[np.argmax(P, axis=1)]
            mse_train[m] = mean_squared_error(y, y_pred_train)
            accuracy_train[m] = accuracy_score(y, y_pred_train)
            log_loss_train[m]=log_loss_value

            # Print progress (optional)
            elapsed_time = time.time() - start_time
            iteration_time = time.time() - iteration_start_time
            estimated_total_time = elapsed_time / (m + 1) * self.n_estimators
            remaining_time = estimated_total_time - elapsed_time
            #print(f"Iteration {m+1}/{self.n_estimators} - Log Loss: {log_loss_value:.4f}, Time Elapsed: {elapsed_time:.2f}s")
            print(f"Iteration {m + 1}/{self.n_estimators} - MSE train: {mse_train[m]:.4f}, Accuracy train: {accuracy_train[m]:.4f},Log Loss: {log_loss_value:.4f}, "
                  f"Time Elapsed: {elapsed_time:.2f}s, Estimated Time Remaining: {remaining_time:.2f}s")

        return mse_train,accuracy_train,log_loss_train

    def predict(self, X, y_test):
        F = np.zeros((X.shape[0], self.K))
        mse_test = np.zeros(self.n_estimators)
        accuracy_test = np.zeros(self.n_estimators)
        log_loss_test = np.zeros(self.n_estimators)
        for i, estimator in enumerate(self.estimators_):
            F += self.learning_rate * estimator.predict_link_scale(X)
            F = np.clip(F, -25, 25)
            P = np.exp(F) / np.exp(F).sum(axis=1, keepdims=True)
            P = np.nan_to_num(P, nan=1e-10)
            y_pred_test = self.classes_[np.argmax(P, axis=1)]
            mse_test[i] = mean_squared_error(y_test, y_pred_test)
            accuracy_test[i] = accuracy_score(y_test, y_pred_test)
            log_loss_test[i] = log_loss(y_test, P)
        return y_pred_test, mse_test, accuracy_test, log_loss_test

    def predict_with_best_model(self, X):
        F = np.zeros((X.shape[0], self.K))
        for estimator in self.estimators_[:self.best_model]:
            F += self.learning_rate * estimator.predict_link_scale(X)
        P = np.exp(F) / np.exp(F).sum(axis=1, keepdims=True)
        return self.classes_[np.argmax(P, axis=1)]